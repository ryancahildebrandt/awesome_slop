{{- define "directory" -}}
## Article Directory
Roughly grouped by study findings as they relate to common LLM claims

- LLMs are biased
	- LLMs are biased towards their own outputs {{ categoryReferences "bias.self" }}
	- LLMs propagate social bias {{ categoryReferences "bias.social" }}
	- LLMs outputs are sycophantic {{ categoryReferences "sycophancy" }}
- LLM evaluations are unreliable
	- Benchmarks aren't enough {{ categoryReferences "eval.bench" }}
	- Hallucination detection is unreliable {{ categoryReferences "eval.hallucination" }}
	- LLMs aren't interpretable {{ categoryReferences "eval.interpret" }}
- LLMs are non-deterministic:
	- 0 Temperature does not ensure determinism {{ categoryReferences "determinism.temp" }}
	- Non-determinism is baked into LLM serving architectures {{ categoryReferences "determinism.serving" }}
- LLM failures are inherent characteristics, not temporary shortcomings
	- Hallucinations aren't solvable {{ categoryReferences "hallucination" }}
	- LLM systems are limited by their architecture {{ categoryReferences "architecture" }}
	- LLMs don't have emergent capabilities {{ categoryReferences "emergent" }}
- LLMs cause harm
	- LLMs violate ethical standards {{ categoryReferences "harm.ethics" }}
	- LLM safeguards are unreliable {{ categoryReferences "harm.safeguards" }}
	- LLMs fail to mitigate harmful user interactions {{ categoryReferences "harm.user" }}
	- LLMs fail to mitigate harmful outputs {{ categoryReferences "harm.output" }}
	- LLMs are bad for the environment {{ categoryReferences "harm.environment" }}
	- LLMs are bad for you {{ categoryReferences "harm.cognitive" }}
	- LLM narratives exacerbate misuse {{ categoryReferences "harm.narrative" }}
	- LLMs aren't secure {{ categoryReferences "harm.security" }}
- LLMs fail
	- LLM outputs are sensitive to input characteristics {{ categoryReferences "input" }}
	- LLMs fail in medical settings {{ categoryReferences "medical" }}
	- LLMs can't report their internal state {{ categoryReferences "selfreport" }}
	- LLMs are not faithful to their inputs {{ categoryReferences "faithfulness" }}
	- LLM based agents do not perform consistently {{ categoryReferences "agents" }}
	- LLMs do not perform consistently {{ categoryReferences "performance" }}
	- LLMs do not utilize context consistently {{ categoryReferences "context" }}
	- LLMs do not follow instructions consistently {{ categoryReferences "instructions" }}
	- LLMs do not reason {{ categoryReferences "reasoning" }}
	- LLMs do not plan {{ categoryReferences "plan" }}
	- LLMs do not research {{ categoryReferences "research" }}
	- LLMs do not calculate {{ categoryReferences "math" }}
	- LLMs do not stand in for humans {{ categoryReferences "human" }}
	- LLMs do not simulate randomness {{ categoryReferences "random" }}
	- LLMs do not show internal consistency {{ categoryReferences "inconsistent" }}
	- LLMs do not have consciousness {{ categoryReferences "conscious" }}
	- LLMs do not have morals {{ categoryReferences "moral" }}
	- LLMs do not understand code {{ categoryReferences "code" }}
	- LLMs do not have a relationship to fact {{ categoryReferences "factual" }}
	- LLMs do not have a relationship to reality {{ categoryReferences "reality" }}
{{- end -}}
